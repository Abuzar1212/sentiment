{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf70667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "file_path = 'Input.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "columns_to_read = ['URL_ID','URL']\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name, usecols=columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ce3478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>50921.0</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>51382.8</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>51844.6</td>\n",
       "      <td>https://insights.blackcoffer.com/what-are-the-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>52306.4</td>\n",
       "      <td>https://insights.blackcoffer.com/marketing-dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>52768.2</td>\n",
       "      <td>https://insights.blackcoffer.com/continued-dem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID                                                URL\n",
       "0      123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1      321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2     2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3     4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4      432.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "..       ...                                                ...\n",
       "109  50921.0  https://insights.blackcoffer.com/coronavirus-i...\n",
       "110  51382.8  https://insights.blackcoffer.com/coronavirus-i...\n",
       "111  51844.6  https://insights.blackcoffer.com/what-are-the-...\n",
       "112  52306.4  https://insights.blackcoffer.com/marketing-dri...\n",
       "113  52768.2  https://insights.blackcoffer.com/continued-dem...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbc00591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_files(filename, mode, data):\n",
    "    with open(filename, mode) as filename:\n",
    "        filename.write(data)\n",
    "        \n",
    "def extract_data(n):\n",
    "    response = requests.get(n)\n",
    "    content = response.text\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    data = []\n",
    "    data.append(soup.title.string)\n",
    "    data.append('\\n')\n",
    "\n",
    "    table = soup.findAll('div', attrs={\"class\": [\"td-post-content tagdiv-type\", \"tdb-block-inner td-fix-index\"]})\n",
    "    for div in table:\n",
    "        p = div.find_all(\"p\")\n",
    "        for i in p:\n",
    "            if i.find_all(\"a\"):\n",
    "                for j in i.find_all(\"a\"):\n",
    "                    j.extract()\n",
    "\n",
    "            if \"disclosure\" not in i.text.lower():\n",
    "                data.append(i.get_text(strip=True))\n",
    "    final_data = \"\"\n",
    "    for n in data:\n",
    "        final_data += n\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d52a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_files():\n",
    "    for (a_id, b_link) in df.itertuples(index=False):\n",
    "        file_link = b_link\n",
    "        new_filename = str(a_id)\n",
    "        new_filename += \".txt\"\n",
    "        with open(new_filename, \"w\", encoding=\"utf-8\") as file_alias:\n",
    "            text_extracted = extract_data(file_link)\n",
    "            file_alias.write(text_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "817fc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d72a67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StopWords_Auditor.txt\") as auditor,open(\"StopWords_Currencies.txt\") as currencies,open(\"StopWords_DatesandNumbers.txt\") as datesandnumbers,open(\"StopWords_Generic.txt\") as generic,open(\"StopWords_GenericLong.txt\") as genericlong,open(\"StopWords_Geographic.txt\") as geographic,open(\"StopWords_Names.txt\") as namess:\n",
    "    data1=auditor.read()\n",
    "    data2=currencies.read()\n",
    "    data3=datesandnumbers.read()\n",
    "    data4=generic.read()\n",
    "    data5=genericlong.read()\n",
    "    data6=geographic.read()\n",
    "    data7=namess.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eea2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "main_list=[]\n",
    "list1=data1.split()\n",
    "list2=data2.split()\n",
    "list3=data3.split()\n",
    "list4=data4.split()\n",
    "list5=data5.split()\n",
    "list6=data6.split()\n",
    "list7=data7.split()\n",
    "\n",
    "for ele in list1:\n",
    "    main_list.append(ele)\n",
    "    \n",
    "for ele in list2:\n",
    "    main_list.append(ele)\n",
    "    \n",
    "for ele in list3:\n",
    "    main_list.append(ele)\n",
    "\n",
    "for ele in list4:\n",
    "    main_list.append(ele)\n",
    "\n",
    "for ele in list5:\n",
    "    main_list.append(ele)\n",
    "    \n",
    "for ele in list6:\n",
    "    main_list.append(ele)\n",
    "    \n",
    "for ele in list7:\n",
    "    main_list.append(ele)\n",
    "\n",
    "    \n",
    "item_to_delete = '|'\n",
    "stop_word_list = [item for item in main_list if item != item_to_delete]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70962432",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive-words.txt','r') as file1,open('negative-words.txt','r') as file2:\n",
    "    pos_list=[]\n",
    "    neg_list=[]\n",
    "    for line in file1:\n",
    "        pos_list.append(line.strip())\n",
    "    for line in file2:\n",
    "        neg_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e576cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dict = {word: True for word in pos_list}\n",
    "negative_dict = {word: False for word in neg_list}\n",
    "sentiment_dict = {**positive_dict, **negative_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df67f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_score(words, word_dict):\n",
    "    \n",
    "    positive_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            positive_score += 1\n",
    "    \n",
    "    return positive_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff2357c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_negative_score(words, word_dict):\n",
    "    \n",
    "    negative_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            negative_score -= 1\n",
    "    \n",
    "    return (negative_score)*-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa7af217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_positive_score(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        text=data.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]','',text)\n",
    "        tokenized_words=word_tokenize(clean_text)\n",
    "        \n",
    "        final_word=[]\n",
    "        for word in tokenized_words:\n",
    "            if word not in stop_word_list:\n",
    "                final_word.append(word)\n",
    "        result=calculate_positive_score(final_word,positive_dict)\n",
    "    \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9233da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_negative_score(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        text=data.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]','',text)\n",
    "        tokenized_words=word_tokenize(clean_text)\n",
    "        \n",
    "        final_word=[]\n",
    "        for word in tokenized_words:\n",
    "            if word not in stop_word_list:\n",
    "                final_word.append(word)\n",
    "        result=calculate_negative_score(final_word,negative_dict)\n",
    "    \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37b50bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_after_cleaning():\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        text=data.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]','',text)\n",
    "        tokenized_words=word_tokenize(clean_text)\n",
    "        \n",
    "        final_word=[]\n",
    "        for word in tokenized_words:\n",
    "            if word not in stop_word_list:\n",
    "                final_word.append(word)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c911a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"POSITIVE SCORE\"]=df[\"URL_ID\"].apply((lambda x : new_positive_score(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea1bf95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NEGATIVE SCORE\"]=df[\"URL_ID\"].apply((lambda x : new_negative_score(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9da2e3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"POLARITY SCORE\"]=(df[\"POSITIVE SCORE\"] - df[\"NEGATIVE SCORE\"])/((df[\"POSITIVE SCORE\"] + df[\"NEGATIVE SCORE\"]) + 0.000001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "617cdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length_in_file(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        words = word_tokenize(data)\n",
    "        total_number_of_words = len(words)\n",
    "        sentences = sent_tokenize(data)\n",
    "        total_number_of_sentences=len(sentences)\n",
    "        average_sentence_length=total_number_of_words/total_number_of_sentences\n",
    "    \n",
    "    return average_sentence_length\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f00f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AVG SENTENCE LENGTH\"]=df[\"URL_ID\"].apply(lambda x : average_sentence_length_in_file(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c4bdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vowels(text):\n",
    "    vowels = \"AEIOUaeiou\"  # List of vowels\n",
    "    vowel_count = 0\n",
    " \n",
    "    for char in text:\n",
    "        if char in vowels:\n",
    "            vowel_count += 1\n",
    "    \n",
    "    if vowel_count > 2 :\n",
    "        return text\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8bb9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_of_complex_words_in_file(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        words=word_tokenize(data)\n",
    "        total_words=len(words)\n",
    "        complex_words=[]\n",
    "        for word in words:\n",
    "            ans=find_vowels(word)\n",
    "            if ans  != -1:\n",
    "                complex_words.append(ans)\n",
    "        \n",
    "        total_complex_words=len(complex_words)\n",
    "        percentage_of_complex_words=total_complex_words/total_words\n",
    "        \n",
    "    \n",
    "    return percentage_of_complex_words\n",
    "                \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "552bf579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PERCENTAGE OF COMPLEX WORDS\"]=df[\"URL_ID\"].apply(lambda x : percentage_of_complex_words_in_file(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e26288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FOG INDEX\"]= 0.4 * (df[\"AVG SENTENCE LENGTH\"] + df['PERCENTAGE OF COMPLEX WORDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b14e6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_no_of_words_per_sentence_in_file(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        words=word_tokenize(data)\n",
    "        total_number_of_words=len(words)\n",
    "        sentences=sent_tokenize(data)\n",
    "        total_sentences=len(sentences)\n",
    "        average_no_of_words_per_sentence=total_number_of_words/total_sentences\n",
    "    \n",
    "    return average_no_of_words_per_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "043e6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AVG NUMBER OF WORDS PER SENTENCE\"]=df[\"URL_ID\"].apply(lambda x : average_no_of_words_per_sentence_in_file(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "104bd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_word_count_in_file(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        words=word_tokenize(data)\n",
    "        total_words=len(words)\n",
    "        complex_words=[]\n",
    "        for word in words:\n",
    "            ans=find_vowels(word)\n",
    "            if ans  != -1:\n",
    "                complex_words.append(ans)\n",
    "        \n",
    "        total_complex_words=len(complex_words)\n",
    "    \n",
    "    return total_complex_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bef3c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"COMPLEX WORD COUNT\"]=df[\"URL_ID\"].apply(lambda x : complex_word_count_in_file(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aae79cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_word_count(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        text=data.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]','',text)\n",
    "        tokenized_words=word_tokenize(clean_text)\n",
    "        \n",
    "        final_word=[]\n",
    "        for word in tokenized_words:\n",
    "            if word not in stop_word_list:\n",
    "                final_word.append(word)\n",
    "        \n",
    "        return len(final_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a68979ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"WORD COUNT\"]=df[\"URL_ID\"].apply(lambda x :cleaned_word_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10430e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllables_per_word_in_file(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        new_words=word_tokenize(data)\n",
    "        total_words=len(new_words)\n",
    "        \n",
    "        new_complex_words1=[]\n",
    "        for word in new_words:\n",
    "            ans=find_vowels(word)\n",
    "            if ans  != -1:\n",
    "                new_complex_words1.append(ans)\n",
    "        \n",
    "        suffixes = (\"es\",\"ed\" )\n",
    "        complex_word_count=[]\n",
    "        for word in new_complex_words1:\n",
    "            if not word.endswith(suffixes):\n",
    "                complex_word_count.append(word)\n",
    "        \n",
    "        syllables_per_word=len(complex_word_count)/total_words\n",
    "        \n",
    "           \n",
    "    \n",
    "    return syllables_per_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "081a352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SYLLABLE PER WORD\"]=df[\"URL_ID\"].apply(lambda x : syllables_per_word_in_file(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a595e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_pronoun_in_file(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        tokens = word_tokenize(data)\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        personal_pronouns = [\"PRP\", \"PRP$\", \"WP\", \"WP$\"]\n",
    "        found_pronouns = []\n",
    "        \n",
    "        for token, tag in tagged_tokens:\n",
    "            if tag in personal_pronouns:\n",
    "                found_pronouns.append(token)\n",
    "        \n",
    "        pronoun_count = len(found_pronouns)\n",
    "        \n",
    "    return pronoun_count\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1f65fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PERSONAL PRONOUNS\"]=df[\"URL_ID\"].apply(lambda x : personal_pronoun_in_file(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b00c41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_count(s):\n",
    "    p=s.split()\n",
    "    count=0\n",
    "    for n in p:\n",
    "        j=len(n)\n",
    "        count+=j\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da944cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length_in_file(file):\n",
    "    x='.txt'\n",
    "    new_name=\"\"\n",
    "    new_name+=str(file)\n",
    "    new_name+=x\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        words=word_tokenize(data)\n",
    "        total_char_count=char_count(data)\n",
    "        avg_word_length=total_char_count/len(words)\n",
    "    \n",
    "    return avg_word_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b485a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG WORD LENGTH']=df[\"URL_ID\"].apply(lambda x : average_word_length_in_file(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce159c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity_score_in_file(file):\n",
    "    with open(new_name,encoding='utf-8') as file:\n",
    "        data=file.read()\n",
    "        text=data.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]','',text)\n",
    "        tokenized_words=word_tokenize(clean_text)\n",
    "        \n",
    "        final_word=[]\n",
    "        for word in tokenized_words:\n",
    "            if word not in stop_word_list:\n",
    "                final_word.append(word)\n",
    "        \n",
    "        total_word=len(final_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95a9fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SUBJECTIVITY SCORE\"]=(df[\"POSITIVE SCORE\"] + df[\"NEGATIVE SCORE\"])/df[\"WORD COUNT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2aeb1a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>71</td>\n",
       "      <td>22</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>31.928571</td>\n",
       "      <td>0.328859</td>\n",
       "      <td>12.902972</td>\n",
       "      <td>31.928571</td>\n",
       "      <td>588</td>\n",
       "      <td>837</td>\n",
       "      <td>0.296421</td>\n",
       "      <td>51</td>\n",
       "      <td>5.316555</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>34.368421</td>\n",
       "      <td>0.326187</td>\n",
       "      <td>13.877843</td>\n",
       "      <td>34.368421</td>\n",
       "      <td>213</td>\n",
       "      <td>286</td>\n",
       "      <td>0.275651</td>\n",
       "      <td>22</td>\n",
       "      <td>5.186830</td>\n",
       "      <td>0.178322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>19.111111</td>\n",
       "      <td>0.273256</td>\n",
       "      <td>7.753747</td>\n",
       "      <td>19.111111</td>\n",
       "      <td>329</td>\n",
       "      <td>544</td>\n",
       "      <td>0.219269</td>\n",
       "      <td>25</td>\n",
       "      <td>4.862957</td>\n",
       "      <td>0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>29.711111</td>\n",
       "      <td>0.305909</td>\n",
       "      <td>12.006808</td>\n",
       "      <td>29.711111</td>\n",
       "      <td>409</td>\n",
       "      <td>639</td>\n",
       "      <td>0.258040</td>\n",
       "      <td>38</td>\n",
       "      <td>5.173523</td>\n",
       "      <td>0.093897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>29.711111</td>\n",
       "      <td>0.305909</td>\n",
       "      <td>12.006808</td>\n",
       "      <td>29.711111</td>\n",
       "      <td>409</td>\n",
       "      <td>639</td>\n",
       "      <td>0.258040</td>\n",
       "      <td>38</td>\n",
       "      <td>5.173523</td>\n",
       "      <td>0.093897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...              71   \n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...              38   \n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...              21   \n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...              33   \n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem...              33   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0              22        0.526882            31.928571   \n",
       "1              13        0.490196            34.368421   \n",
       "2              27       -0.125000            19.111111   \n",
       "3              27        0.100000            29.711111   \n",
       "4              27        0.100000            29.711111   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                     0.328859  12.902972                         31.928571   \n",
       "1                     0.326187  13.877843                         34.368421   \n",
       "2                     0.273256   7.753747                         19.111111   \n",
       "3                     0.305909  12.006808                         29.711111   \n",
       "4                     0.305909  12.006808                         29.711111   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                 588         837           0.296421                 51   \n",
       "1                 213         286           0.275651                 22   \n",
       "2                 329         544           0.219269                 25   \n",
       "3                 409         639           0.258040                 38   \n",
       "4                 409         639           0.258040                 38   \n",
       "\n",
       "   AVG WORD LENGTH  SUBJECTIVITY SCORE  \n",
       "0         5.316555            0.111111  \n",
       "1         5.186830            0.178322  \n",
       "2         4.862957            0.088235  \n",
       "3         5.173523            0.093897  \n",
       "4         5.173523            0.093897  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14372c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(columns=['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed1c134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"assign_file_new.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203d422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88c71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
